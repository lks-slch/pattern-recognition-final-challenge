{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dfcd720-f339-49c8-9a9d-d7acf8a38cd6",
   "metadata": {},
   "source": [
    "# Task 4: The Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7951a-acb4-438b-8098-2de4dd3bd42b",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "482b5489-fbd3-4a8d-be68-b48702ca862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, auc, confusion_matrix, make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24331be3-4995-4772-bd8b-c44b1c36eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "df_train = pd.read_csv(\"task_3_training_e8da4715deef7d56_f8b7378_generic.csv\", header=0)\n",
    "df_test = pd.read_csv(\"task_4_test_dd4bd32b08b776e6_daf99ad_generic.csv\", header=0)\n",
    "\n",
    "# keep high level features (GEMS) as separate array for training\n",
    "\n",
    "hl_train = df_train[df_train.columns[175:]]\n",
    "\n",
    "# drop columns 175-201, since they contain the high-level features not available in the test set\n",
    "\n",
    "df_train = df_train.drop(df_train.columns[175:], axis=1)\n",
    "\n",
    "# drop valence and arousal as specified in the task description as well as pianist, segment and snippet column\n",
    "\n",
    "df_train = df_train.drop([\"arousal\", \"valence\", \"pianist_id\", \"segment_id\", \"snippet_id\"], axis=1)\n",
    "df_test = df_test.drop([\"pianist_id\", \"segment_id\", \"snippet_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08bf83a5-2d0d-4b12-b861-c278655f3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data and labels from training set\n",
    "\n",
    "X_train = df_train[df_train.columns[:-1]]\n",
    "y_train = df_train[df_train.columns[-1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8080a71-f02e-4d5e-a669-ce411cd08420",
   "metadata": {},
   "source": [
    "## Predicting high-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f01229a-3780-41c3-b854-3c90d208a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(df_test)\n",
    "\n",
    "# use non-optimized xgb to predict high level features\n",
    "# multioutputregressor trains one regressor for each high level feature\n",
    "\n",
    "xgb_multi = MultiOutputRegressor(xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42), n_jobs=-1)\n",
    "xgb_multi.fit(X_train, hl_train)\n",
    "hl_pred = xgb_multi.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ab5d7d-492b-43fb-8cbd-fc45bb21c4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.9042423 , 3.0406234 , 1.4873655 , ..., 0.10163505, 0.04431162,\n",
       "        0.03267397],\n",
       "       [3.0142393 , 2.9344585 , 1.86377   , ..., 0.01307283, 0.0660662 ,\n",
       "        0.34811604],\n",
       "       [2.512981  , 2.1732771 , 1.7583784 , ..., 0.04756289, 0.37012482,\n",
       "        0.02735191],\n",
       "       ...,\n",
       "       [2.9266903 , 2.861176  , 1.5889468 , ..., 0.07562069, 0.24788648,\n",
       "        0.07221433],\n",
       "       [2.5819573 , 2.550523  , 1.387153  , ..., 0.26977864, 0.7670786 ,\n",
       "        0.04646803],\n",
       "       [2.8836863 , 3.0191398 , 1.3788074 , ..., 0.29734248, 0.7923114 ,\n",
       "        0.13583864]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2abcd0e0-ee6b-454d-8fa9-03da3d27964b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hl_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# merge high level features together with test set\u001b[39;00m\n\u001b[0;32m      2\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_wonder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_transcendence\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_tenderness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_nostalgia\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_peacefulness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_power\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_joyful_activation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_tension\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_sadness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_movement\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_force\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_interior\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_wandering\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_flow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_wonder_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_transcendence_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_tenderness_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_nostalgia_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_peacefulness_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_power_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_joyful_activation_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_tension_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgems_sadness_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_movement_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_force_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_interior_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_wandering_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemmes_flow_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_test, pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mhl_pred\u001b[49m, columns\u001b[38;5;241m=\u001b[39mcolumns)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m df_test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_test_restored_hl.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m df_test\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hl_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# merge high level features together with test set\n",
    "columns = [\"gems_wonder\",\"gems_transcendence\",\"gems_tenderness\",\"gems_nostalgia\",\"gems_peacefulness\",\"gems_power\",\"gems_joyful_activation\",\"gems_tension\",\"gems_sadness\",\"gemmes_movement\",\"gemmes_force\",\"gemmes_interior\",\"gemmes_wandering\",\"gemmes_flow\",\"gems_wonder_binary\",\"gems_transcendence_binary\",\"gems_tenderness_binary\",\"gems_nostalgia_binary\",\"gems_peacefulness_binary\",\"gems_power_binary\",\"gems_joyful_activation_binary\",\"gems_tension_binary\",\"gems_sadness_binary\",\"gemmes_movement_binary\",\"gemmes_force_binary\",\"gemmes_interior_binary\",\"gemmes_wandering_binary\",\"gemmes_flow_binary\"]\n",
    "df_test = pd.concat([df_test, pd.DataFrame(hl_pred, columns=columns)], axis=1)\n",
    "df_test.to_csv(\"df_test_restored_hl.csv\", index=False)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e2a23-e2d1-4ccb-815d-a7bf8ab909a5",
   "metadata": {},
   "source": [
    "## Predict quadrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "685f09ec-a354-4fde-9248-d5ca0138e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload df_train to restore high level features removed earlier and drop non usable columns and get df_test from saved file (so the notebook doesn't need to be run every time)\n",
    "\n",
    "df_train = pd.read_csv(\"task_3_training_e8da4715deef7d56_f8b7378_generic.csv\", header=0)\n",
    "df_test = pd.read_csv(\"df_test_restored_hl.csv\", header=0)\n",
    "\n",
    "# save segment_id column for GroupShuffleSplit and drop pianist, segment and snippet column\n",
    "group_idx = df_train[\"segment_id\"]\n",
    "\n",
    "df_train = df_train.drop([\"arousal\", \"valence\", \"pianist_id\", \"segment_id\", \"snippet_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa0b5700-a903-47e1-a065-71179929b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last 14 columns (GEMS) are binary. So far we predicted values in range [0,1]. We round the values to match the binary type\n",
    "\n",
    "df_test[df_test.columns[-14:]] = df_test[df_test.columns[-14:]].round(decimals=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a2e05ec-eace-43ea-9bb9-e46543a0d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data and label\n",
    "\n",
    "X = df_train.drop([\"quadrant\"], axis=1)\n",
    "y = df_train[\"quadrant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7daf9c97-66d1-4811-8198-851cfd106607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom scoring function for GridSearchCV\n",
    "\n",
    "# Labels and corresponding mood\n",
    "# 1: happy\n",
    "# 2: angry\n",
    "# 3: sad\n",
    "# 4: relaxed\n",
    "\n",
    "def filmotion_scoring(y, y_pred):\n",
    "    balance = 0\n",
    "    revenue_matrix = np.array([[5,-5,-5,2],[-5,10,2,-5],[-5,2,10,-5],[2,-5,-2,5]])\n",
    "    for true, pred in zip(y, y_pred):\n",
    "        balance += revenue_matrix[int(true)-1,int(pred)-1]\n",
    "    return balance/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae640468-5d0e-4772-b2b5-afbb23ec869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.logical_or(np.logical_or(group_idx==20, group_idx==21), group_idx==23)\n",
    "\n",
    "# define test set\n",
    "X_test = X.loc[mask]\n",
    "y_test = y.loc[mask]\n",
    "group_idx_test = group_idx.loc[mask]\n",
    "\n",
    "# define training set\n",
    "mask = np.invert(mask)\n",
    "X_train = X.loc[mask]\n",
    "y_train = y.loc[mask]\n",
    "group_idx_train = group_idx.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4c1de52-6a2e-42f3-8e7c-8b6a49444afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use GroupKFold which seperates our data into k folds in which the groups differ, in a way that in \n",
    "# the first fold only group 1,2 in the second fold only group 3,4 ... are listed\n",
    "gkf = GroupKFold(n_splits=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26fbc05a-6b6e-4d88-b6d9-7e32978e0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make custom scorer for grid search using previously defined scoring function\n",
    "filmotion = make_scorer(filmotion_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a2efc6d-cd80-4d58-938e-43bf7c1902fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#########\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m## XGB ##\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#########\u001b[39;00m\n\u001b[0;32m     16\u001b[0m xgb_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_child_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m4\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m4\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m800\u001b[39m]}\n\u001b[1;32m---> 18\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39m\u001b[43mXGBClassifier\u001b[49m(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerror\u001b[39m\u001b[38;5;124m'\u001b[39m), param_grid\u001b[38;5;241m=\u001b[39mxgb_params, cv\u001b[38;5;241m=\u001b[39mgkf, scoring\u001b[38;5;241m=\u001b[39mfilmotion)\n\u001b[0;32m     19\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, groups\u001b[38;5;241m=\u001b[39mgroup_idx_train)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m########\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m## RF ##\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m########\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Getting the best parameter settings using GridSearch\n",
    "\n",
    "#########\n",
    "## KNN ##\n",
    "#########\n",
    "\n",
    "knn_params = {\"n_neighbors\" : [3, 5, 25, 50], \"algorithm\": [\"auto\"], \"weights\": [\"uniform\"]}\n",
    "\n",
    "knn_model = GridSearchCV(KNeighborsClassifier(), param_grid=knn_params, cv=gkf, scoring=filmotion)\n",
    "knn_model.fit(X_train, y_train, groups=group_idx_train)\n",
    "\n",
    "#########\n",
    "## XGB ##\n",
    "#########\n",
    "\n",
    "xgb_params = {'min_child_weight': [4], 'gamma': [0], 'max_depth': [4],'n_estimators': [800]}\n",
    "\n",
    "xgb_model = GridSearchCV(estimator=XGBClassifier(use_label_encoder=False, eval_metric='merror'), param_grid=xgb_params, cv=gkf, scoring=filmotion)\n",
    "xgb_model.fit(X_train, y_train, groups=group_idx_train)\n",
    "\n",
    "########\n",
    "## RF ##\n",
    "########\n",
    "\n",
    "rf_params = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf_model = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rf_params, cv=gkf, scoring=filmotion)\n",
    "rf_model.fit(X_train, y_train, groups=group_idx_train)\n",
    "\n",
    "#########################\n",
    "## Logistic Regression ##\n",
    "#########################\n",
    "\n",
    "log_reg_param = {\"penalty\": ['l2'], 'C': [1], \"dual\":[True]}\n",
    "reg_model = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=8000), param_grid=log_reg_param, cv=gkf, scoring=filmotion)\n",
    "reg_model.fit(X_train, y_train, groups=group_idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4dba2d-c6c3-4507-b8cc-967e63b428ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
