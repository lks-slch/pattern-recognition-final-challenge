{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dfcd720-f339-49c8-9a9d-d7acf8a38cd6",
   "metadata": {},
   "source": [
    "# Task 4: The Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7951a-acb4-438b-8098-2de4dd3bd42b",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482b5489-fbd3-4a8d-be68-b48702ca862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, auc, confusion_matrix, make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf55b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, ParameterGrid\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24331be3-4995-4772-bd8b-c44b1c36eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "df_train = pd.read_csv(\"task_3_training_e8da4715deef7d56_f8b7378_generic.csv\", header=0)\n",
    "df_test = pd.read_csv(\"task_4_test_dd4bd32b08b776e6_daf99ad_generic.csv\", header=0)\n",
    "\n",
    "# keep high level features (GEMS) as separate array for training\n",
    "\n",
    "hl_train = df_train[df_train.columns[175:]]\n",
    "\n",
    "# drop columns 175-201, since they contain the high-level features not available in the test set\n",
    "\n",
    "df_train = df_train.drop(df_train.columns[175:], axis=1)\n",
    "\n",
    "# drop valence and arousal as specified in the task description as well as pianist, segment and snippet column\n",
    "\n",
    "df_train = df_train.drop([\"arousal\", \"valence\", \"pianist_id\", \"segment_id\", \"snippet_id\"], axis=1)\n",
    "df_test_ids = df_test[[\"pianist_id\", \"segment_id\", \"snippet_id\"]]\n",
    "df_test = df_test.drop([\"pianist_id\", \"segment_id\", \"snippet_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb3ffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2085, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561fa778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2085, 169), (2282, 170))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08bf83a5-2d0d-4b12-b861-c278655f3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data and labels from training set\n",
    "\n",
    "X_train = df_train[df_train.columns[:-1]]\n",
    "y_train = df_train[df_train.columns[-1]]\n",
    "\n",
    "# this adjustment is because of the XGBoost, it needs value for multiclass in 0... num of class - 1\n",
    "y_train = y_train -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8080a71-f02e-4d5e-a669-ce411cd08420",
   "metadata": {},
   "source": [
    "## Predicting high-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01229a-3780-41c3-b854-3c90d208a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(df_test)\n",
    "\n",
    "# use non-optimized xgb to predict high level features\n",
    "# multioutputregressor trains one regressor for each high level feature\n",
    "\n",
    "xgb_multi = MultiOutputRegressor(xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42), n_jobs=-1)\n",
    "xgb_multi.fit(X_train, hl_train)\n",
    "hl_pred = xgb_multi.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab5d7d-492b-43fb-8cbd-fc45bb21c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcd0e0-ee6b-454d-8fa9-03da3d27964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge high level features together with test set\n",
    "columns = [\"gems_wonder\",\"gems_transcendence\",\"gems_tenderness\",\"gems_nostalgia\",\"gems_peacefulness\",\"gems_power\",\"gems_joyful_activation\",\"gems_tension\",\"gems_sadness\",\"gemmes_movement\",\"gemmes_force\",\"gemmes_interior\",\"gemmes_wandering\",\"gemmes_flow\",\"gems_wonder_binary\",\"gems_transcendence_binary\",\"gems_tenderness_binary\",\"gems_nostalgia_binary\",\"gems_peacefulness_binary\",\"gems_power_binary\",\"gems_joyful_activation_binary\",\"gems_tension_binary\",\"gems_sadness_binary\",\"gemmes_movement_binary\",\"gemmes_force_binary\",\"gemmes_interior_binary\",\"gemmes_wandering_binary\",\"gemmes_flow_binary\"]\n",
    "df_test = pd.concat([df_test, pd.DataFrame(hl_pred, columns=columns)], axis=1)\n",
    "df_test.to_csv(\"df_test_restored_hl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41001bf",
   "metadata": {},
   "source": [
    "# df_test exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 15.5)\n",
    "sub_df = df_test.astype('float64')\n",
    "l = sub_df.columns.shape[0]\n",
    "sub_df.hist(figsize=(40,70),layout=( 57 ,4),color='indigo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7134f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating pearson correlations and taking only pair where significant noticed\n",
    "corr = df_test.astype('float64'\n",
    "                   ).corr(method='pearson')\n",
    "redundant = []\n",
    "for column in corr.columns:\n",
    "    n_larg = corr[column].nlargest(3)\n",
    "    n_smal = corr[column].nsmallest(3)\n",
    "    if abs(n_larg[1])>0.75:\n",
    "        print(f\"{'='*60}\\nFeature {column} is most correlated with\\n\")\n",
    "        print(n_larg)\n",
    "        if abs(n_smal[0])>0.75:\n",
    "            print(n_smal)\n",
    "    elif abs(n_smal[1])>0.75:\n",
    "        print(f\"{'='*60}\\nFeature {column} is most correlated with\\n\")\n",
    "        print(n_smal)\n",
    "    else:\n",
    "        redundant.append(column)\n",
    "print(f\"{'='*60}\\nFollowing fetures have no significant correlations:\\n{redundant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0259e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_abs_correlations(df_test, n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e2a23-e2d1-4ccb-815d-a7bf8ab909a5",
   "metadata": {},
   "source": [
    "## Predict quadrants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db05d8d",
   "metadata": {},
   "source": [
    "# There shoul be saciling but i discovered it lately. It works even without, but classifiers,\n",
    "# than classiers are confused. But they work, not best.\n",
    "# Never mind, do not care about it, do only what is mandatory for us. They do not check a code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a57317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload df_train to restore high level features removed earlier and drop non usable columns and get df_test from saved file (so the notebook doesn't need to be run every time)\n",
    "\n",
    "df_train = pd.read_csv(\"task_3_training_e8da4715deef7d56_f8b7378_generic.csv\", header=0)\n",
    "df_test = pd.read_csv(\"df_test_restored_hl.csv\", header=0)\n",
    "\n",
    "#There shoul be saciling but i discovered it lately. It works even without, but classifiers,\n",
    "# than classiers are confused. But they work, not best.\n",
    "# Never mind, do not care about it, do only what is mandatory for us. They do not check a code.\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "\n",
    "# save segment_id column for GroupShuffleSplit and drop pianist, segment and snippet column\n",
    "group_idx = df_train[\"segment_id\"]\n",
    "\n",
    "df_train = df_train.drop([\"arousal\", \"valence\", \"pianist_id\", \"segment_id\", \"snippet_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b5700-a903-47e1-a065-71179929b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last 14 columns (GEMS) are binary. So far we predicted values in range [0,1]. We round the values to match the binary type\n",
    "\n",
    "df_test[df_test.columns[-14:]] = df_test[df_test.columns[-14:]].round(decimals=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e05ec-eace-43ea-9bb9-e46543a0d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data and label\n",
    "\n",
    "X = df_train.drop([\"quadrant\"], axis=1)\n",
    "y = df_train[\"quadrant\"]\n",
    "\n",
    "# this adjustment is because of the XGBoost, it needs value for multiclass in 0... num of class - 1\n",
    "y = y -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf9c97-66d1-4811-8198-851cfd106607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom scoring function for GridSearchCV\n",
    "\n",
    "# Labels and corresponding mood\n",
    "# 1: happy\n",
    "# 2: angry\n",
    "# 3: sad\n",
    "# 4: relaxed\n",
    "\n",
    "def filmotion_scoring(y, y_pred):\n",
    "    balance = 0\n",
    "    revenue_matrix = np.array([[5,-5,-5,2],[-5,10,2,-5],[-5,2,10,-5],[2,-5,-2,5]])\n",
    "    for true, pred in zip(y, y_pred):\n",
    "        balance += revenue_matrix[int(true)-1,int(pred)-1]\n",
    "    return balance/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae640468-5d0e-4772-b2b5-afbb23ec869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.logical_or(np.logical_or(group_idx==20, group_idx==21), group_idx==23)\n",
    "\n",
    "# define validation set\n",
    "X_valid = X.loc[mask]\n",
    "y_valid = y.loc[mask]\n",
    "group_idx_valid = group_idx.loc[mask]\n",
    "\n",
    "# define training set\n",
    "mask = np.invert(mask)\n",
    "X_train = X.loc[mask]\n",
    "y_train = y.loc[mask]\n",
    "group_idx_train = group_idx.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1de52-6a2e-42f3-8e7c-8b6a49444afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use GroupKFold which seperates our data into k folds in which the groups differ, in a way that in \n",
    "# the first fold only group 1,2 in the second fold only group 3,4 ... are listed\n",
    "gkf = GroupKFold(n_splits=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fbc05a-6b6e-4d88-b6d9-7e32978e0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make custom scorer for grid search using previously defined scoring function\n",
    "filmotion = make_scorer(filmotion_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_matric(yTest, yPred, write=True):\n",
    "    c_matrix = confusion_matrix(yTest, yPred)\n",
    "    tmp = 0 \n",
    "    for i in range(4):\n",
    "        tmp += c_matrix[i,i] / sum(c_matrix[i,:])\n",
    "        if write:\n",
    "            print(f'sensitivity of classier  {i}. is: {c_matrix[i,i] / sum(c_matrix[i,:])}' )\n",
    "    return tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2efc6d-cd80-4d58-938e-43bf7c1902fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the best parameter settings using GridSearch\n",
    "\n",
    "#########\n",
    "## KNN ##\n",
    "#########\n",
    "\n",
    "knn_params = {\"n_neighbors\" : [3, 5, 25, 50], \"algorithm\": [\"auto\"], \"weights\": [\"uniform\"]}\n",
    "\n",
    "knn_model = GridSearchCV(KNeighborsClassifier(), param_grid=knn_params, cv=gkf, scoring=filmotion)\n",
    "knn_model.fit(X_train, y_train, groups=group_idx_train)\n",
    "knn_param = knn_model.best_params_\n",
    "knn_model = knn_model.best_estimator_\n",
    "print(\"KNN has sensitivity\")\n",
    "knn_prob = knn_model.predict(X_valid)\n",
    "sensitivity_matric(y_valid, knn_prob)\n",
    "print()\n",
    "\n",
    "\n",
    "#########\n",
    "## XGB ##\n",
    "#########\n",
    "\n",
    "xgb_params = {'min_child_weight': [4], 'gamma': [0], 'max_depth': [4],\n",
    "             'n_estimators': [800]}\n",
    "\n",
    "xgb_model = GridSearchCV(estimator=XGBClassifier(use_label_encoder=False, eval_metric='merror'), param_grid=xgb_params,\n",
    "                         cv=gkf)\n",
    "xgb_model.fit(X_train, y_train, groups=group_idx_train)\n",
    "xgb_param = xgb_model.best_params_\n",
    "xgb_model = xgb_model.best_estimator_\n",
    "\n",
    "print(\"XGBoost has sensitivity\")\n",
    "xgb_prob = xgb_model.predict(X_valid)\n",
    "sensitivity_matric(y_valid, xgb_prob)\n",
    "print()\n",
    "########\n",
    "## RF ##\n",
    "########\n",
    "\n",
    "rf_params = { \n",
    "    'n_estimators': [200],\n",
    "    'max_features': ['auto'],\n",
    "    'max_depth' : [4,],\n",
    "    'criterion' :['gini']\n",
    "}\n",
    "\n",
    "rf_model = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rf_params, cv=gkf, scoring=filmotion)\n",
    "rf_model.fit(X_train, y_train, groups=group_idx_train)\n",
    "rf_param = rf_model.best_params_\n",
    "rf_model = rf_model.best_estimator_\n",
    "print(\"RandomForest has sensitivity\")\n",
    "rf_prob = rf_model.predict(X_valid)\n",
    "sensitivity_matric(y_valid, rf_prob)\n",
    "print()\n",
    "\n",
    "#########################\n",
    "## Logistic Regression ##\n",
    "#########################\n",
    "\n",
    "log_reg_param = {\"penalty\": ['l2'], 'C': [1], \"dual\":[True]}\n",
    "reg_model = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=8000), param_grid=log_reg_param, cv=gkf, scoring=filmotion)\n",
    "reg_model.fit(X_train, y_train, groups=group_idx_train)\n",
    "reg_param = reg_model.best_params_\n",
    "reg_model = reg_model.best_estimator_\n",
    "print(\"LogisticRegression has sensitivity\")\n",
    "reg_prob = reg_model.predict(X_valid)\n",
    "sensitivity_matric(y_valid, reg_prob)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4dba2d-c6c3-4507-b8cc-967e63b428ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LogisiticRegression has param:\", reg_param)\n",
    "print(\"KNerest has param:\", knn_param)\n",
    "print(\"DecisionTreeClassifier has param:\", rf_param)\n",
    "print(\"XGBClassifier has param:\", xgb_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_param[\"solver\"] = 'liblinear'\n",
    "xgb_param[\"use_label_encoder\"] = False\n",
    "xgb_param[\"objective\"] = \"multi:softmax\"\n",
    "xgb_param[\"num_class\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "importance_dataset = pd.DataFrame({'Features': X.columns, 'Feature importance': rf_model.feature_importances_})\n",
    "importance_dataset = importance_dataset.sort_values(by=['Feature importance'],ascending=False).reset_index(drop=True)\n",
    "importance_dataset = importance_dataset[:20]\n",
    "plt.title('RandomForestClassifier features importance', size=20)\n",
    "ax = sns.barplot(x=\"Features\", y=\"Feature importance\", data=importance_dataset)\n",
    "\n",
    "ax.set_xlabel(\"Features\",fontsize=10)\n",
    "ax.tick_params(axis='x', labelsize=10, rotation=90)\n",
    "ax.set_ylabel(\"Feature importance\",fontsize=20)\n",
    "ax.tick_params(axis='y', labelsize=10, rotation=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d274c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "importance_dataset = pd.DataFrame({'Features': X.columns, 'Feature importance': xgb_model.feature_importances_})\n",
    "importance_dataset = importance_dataset.sort_values(by=['Feature importance'],ascending=False).reset_index(drop=True)\n",
    "importance_dataset = importance_dataset[:20]\n",
    "plt.title('XGBClassifier feature importance', size=20)\n",
    "ax = sns.barplot(x=\"Features\", y=\"Feature importance\", data=importance_dataset)\n",
    "\n",
    "ax.set_xlabel(\"Features\",fontsize=10)\n",
    "ax.tick_params(axis='x', labelsize=10, rotation=90)\n",
    "ax.set_ylabel(\"Feature importance\",fontsize=20)\n",
    "ax.tick_params(axis='y', labelsize=10, rotation=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "importance = reg_model.coef_[0]\n",
    "columns = X.columns\n",
    "\n",
    "im_df = pd.DataFrame({\"Feature importance\": importance, \"Features\": columns })\n",
    "\n",
    "print((im_df[\"Feature importance\"].abs() > 0.0003).value_counts())\n",
    "im_df = im_df[im_df[\"Feature importance\"].abs() > 0.0003]\n",
    "plt.title('LogisticRegressor feature importance', size=20)\n",
    "ax = sns.barplot(x=\"Features\", y=\"Feature importance\", data=im_df)\n",
    "\n",
    "ax.set_xlabel(\"Features\",fontsize=10)\n",
    "ax.tick_params(axis='x', labelsize=10, rotation=90)\n",
    "ax.set_ylabel(\"Feature importance\",fontsize=20)\n",
    "ax.tick_params(axis='y', labelsize=10, rotation=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d374c12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The ensemble works similar to gradient descend. It tries to find the global optimum for coefficients. So we want to find the best weight for each model, that gives us the best result by summing the result of multiplying weights and model prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, weights, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = np.array(yhats)\n",
    "    # weighted sum across ensemble members\n",
    "    summed = np.tensordot(yhats, weights, axes=((0),(0)))\n",
    "    return summed.round()\n",
    "\n",
    "# # evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(members, weights, testX)\n",
    "    # calculate AUC\n",
    "   \n",
    "    \n",
    "    return sensitivity_matric(testy, yhat, write=False)\n",
    "\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "\n",
    "# loss function for optimization process, designed to be minimized\n",
    "def loss_function(weights, members, testX, testy):\n",
    "    # normalize weights\n",
    "    normalized = normalize(weights)\n",
    "    # calculate error rate\n",
    "    return 1.0 - evaluate_ensemble(members, normalized, testX, testy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = X_valid\n",
    "testy = y_valid\n",
    "\n",
    "members = [reg_model, knn_model, rf_model, xgb_model]\n",
    "\n",
    "for i in range(len(members)):\n",
    "    test_prob = members[i].predict(testX)\n",
    "   \n",
    "    print('Model %d: %.3f' % (i+1, accuracy_score(testy, test_prob)))\n",
    "    \n",
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0/len(members) for _ in range(len(members))]\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "\n",
    "\n",
    "print('Equal Weights Score: %.3f' % score)\n",
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0)  for _ in range(len(members))]\n",
    "# arguments to the loss function\n",
    "search_arg = (members, testX, testy)\n",
    "# global optimization of ensemble weights\n",
    "\n",
    "result = differential_evolution(loss_function, bound_w, search_arg, maxiter=300, tol=1e-7)\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "print('Optimized Weights: %s' % weights)\n",
    "# evaluate chosen weights\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Optimized Weights AUC: %.3f' % score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "print('Optimized Weights: %s' % weights)\n",
    "# evaluate chosen weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c4f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a678ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "#     axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                          train_scores_mean + train_scores_std, alpha=0.1,\n",
    "#                          color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "#     axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "#                  label=\"Training score\")\n",
    "  \n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(LogisticRegression(**reg_param), \"LogisticRegression model\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(RandomForestClassifier(**rf_param), \"Random Forest model\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7777b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(XGBClassifier(**xgb_param), \"XGBoost model\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb488bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_learning_curve(KNeighborsClassifier(**knn_param), \"KNN model\", X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(testY, predY, key=\"name\"):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    cmp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix(testY, predY),\n",
    "        display_labels=[\"class_1\", \"class_2\", \"class_3\", \"class_4\"]\n",
    "        )\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.title(key, size=20)\n",
    "    cmp.plot(ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b884540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_valid, reg_model.predict(X_valid), key=\"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39251e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_valid, knn_model.predict(X_valid), key=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22667688",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_valid, rf_model.predict(X_valid), key=\"RandomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf46783",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_valid, xgb_model.predict(X_valid), key=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_valid, ensemble_predictions(members, weights, X_valid), key=\"Ensemble model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64632e45",
   "metadata": {},
   "source": [
    "# Final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efbc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat = pd.concat([X_valid, X_train], axis = 0)\n",
    "y_train_concat = pd.concat([y_valid, y_train], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500561a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79388f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LogisticRegression(**reg_param)\n",
    "rf_model = RandomForestClassifier(**rf_param)\n",
    "xgb_model = XGBClassifier(**xgb_param)\n",
    "knn_model = KNeighborsClassifier(**knn_param)\n",
    "\n",
    "reg_model.fit(X_train_concat, y_train_concat)\n",
    "rf_model.fit(X_train_concat, y_train_concat)\n",
    "xgb_model.fit(X_train_concat, y_train_concat)\n",
    "knn_model.fit(X_train_concat, y_train_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final prediction\n",
    "members = [reg_model, knn_model, rf_model, xgb_model]\n",
    "\n",
    "y_test = ensemble_predictions(members, weights, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed459c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we did this because of xgboost, but now we need adjust it again \n",
    "y_test = y_test + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786cd718",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.Series(y_test)\n",
    "y_test = pd.concat([df_test_ids ,y_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.columns = [\"pianist_id\", \"segment_id\", \"snippet_id\", \"quadrant\"]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf20c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
